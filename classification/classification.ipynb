{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARDISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_dataframe(dataframe):\n",
    "    # Sélectionner uniquement les colonnes numériques\n",
    "    numeric_cols = dataframe.select_dtypes(include=['float64', 'int64','int32']).columns\n",
    "    # Copier le DataFrame pour éviter de modifier l'original\n",
    "    standardized_df = dataframe.copy()\n",
    "    # Standardiser les colonnes numériques\n",
    "    scaler = StandardScaler()\n",
    "    standardized_df[numeric_cols] = scaler.fit_transform(dataframe[numeric_cols])\n",
    "    \n",
    "    return standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximisation_marge(montant,status):\n",
    "    taux_marge = 0.05\n",
    "    if status == \"TP\":\n",
    "        res = 0 # le fraudeur est détecté\n",
    "    elif status == \"TN\":\n",
    "        res = taux_marge * montant # un client honnête est accepté\n",
    "    elif status == \"FP\":\n",
    "        res = 0.7*taux_marge * montant # un client honnête est bloqué     \n",
    "    elif status == \"FN\": # un fraudeur est accepté \n",
    "        if montant <= 20:\n",
    "            res = 0\n",
    "        elif montant <= 50:\n",
    "            res = -0.2 * montant\n",
    "        elif montant <= 100:\n",
    "            res = -0.3 * montant\n",
    "        elif montant <= 200:\n",
    "            res = -0.5 * montant\n",
    "        else:\n",
    "            res = -0.8 * montant\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATION - SCORER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(y_true, y_pred, montants):\n",
    "    total_marge = 0\n",
    "    inde = 0\n",
    "    for i in range(len(montants)):\n",
    "        status = \"\"\n",
    "        if y_true[inde] == 1 and y_pred[inde] == 1:\n",
    "            status = \"TP\"\n",
    "        elif y_true[inde] == 0 and y_pred[inde] == 0:\n",
    "            status = \"TN\"\n",
    "        elif y_true[inde] == 0 and y_pred[inde] == 1:\n",
    "            status = \"FP\"\n",
    "        elif y_true[inde] == 1 and y_pred[inde] == 0:\n",
    "            status = \"FN\" \n",
    "\n",
    "        total_marge += maximisation_marge(montants[inde], status)\n",
    "        inde += 1\n",
    "    \n",
    "    return total_marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATION - CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Support_Vector_Machine': SVC(), 'Linear_Discriminant_Analysis': LinearDiscriminantAnalysis()}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "class_weights = {0: 1.0, 1: 20.0}\n",
    "\n",
    "models = {\n",
    "    #'Random_Forest': RandomForestClassifier(), # LUI\n",
    "    #'xgb_model': xgb.XGBClassifier(), # LUI\n",
    "    #'Gradient_Boosting': GradientBoostingClassifier(),\n",
    "    'Support_Vector_Machine': SVC(), # LUI\n",
    "    'Linear_Discriminant_Analysis': LinearDiscriminantAnalysis() # LUI\n",
    "    #'Logistic_Regression': LogisticRegression()\n",
    "}\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Random_Forest': {\n",
    "        'criterion': ['gini'],\n",
    "        'n_estimators': [5,100],\n",
    "        'max_depth': [None, 10],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'random_state': [42]\n",
    "        },\n",
    "    'xgb_model': {\n",
    "        'objective': ['binary:logistic'],\n",
    "        'n_estimators': [5,100],\n",
    "        #'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [None,3,10],\n",
    "        'subsample': [0.5,1.0],\n",
    "        #'colsample_bytree': [0.8, 1.0],\n",
    "        #'gamma': [0, 0.1, 0.2],\n",
    "        #'min_child_weight': [1, 5, 10],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'Gradient_Boosting': {\n",
    "        #'n_estimators': [50, 100, 200],\n",
    "        #'learning_rate': [0.01, 0.1, 0.2],\n",
    "        #'max_depth': [3, 5, 7],\n",
    "        #'subsample': [0.8, 1.0],\n",
    "        #'min_samples_split': [2, 5, 10],\n",
    "        #'min_samples_leaf': [1, 2, 4],\n",
    "        #'max_features': [None, 'sqrt', 'log2'],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'K_Nearest_Neighbors': {\n",
    "        'n_neighbors': [3,10],\n",
    "        #'weights': ['uniform', 'distance'],\n",
    "        #'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        #'leaf_size': [20, 30, 40],\n",
    "        #'p': [1, 2]\n",
    "    },\n",
    "    'Support_Vector_Machine': {\n",
    "        'C': [0.1, 1.0, 5.0],\n",
    "        'kernel': ['linear','rbf'],\n",
    "        #'degree': [2, 3, 4],\n",
    "        #'gamma': ['scale', 'auto'],\n",
    "        'class_weight': [None,class_weights],\n",
    "        'random_state': [42]    \n",
    "       },\n",
    "   'Neural_Network': {\n",
    "        #'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "        #'activation': ['relu', 'tanh', 'logistic'],\n",
    "        #'solver': ['sgd', 'adam'],\n",
    "        #'alpha': [0.0001, 0.001, 0.01],\n",
    "        #'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        #'max_iter': [100, 200, 300],\n",
    "        #'early_stopping': [True, False],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'Linear_Discriminant_Analysis': {\n",
    "        'solver': ['svd', 'lsqr'],\n",
    "        'shrinkage': [None, 'auto']\n",
    "        #'n_components': [None, 1, 2, 3]\n",
    "    },\n",
    "     'Logistic_Regression': {\n",
    "        #'penalty': ['l1', 'l2'],\n",
    "        #'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        #'fit_intercept': [True, False],\n",
    "        #'class_weight': [None, 'balanced'],\n",
    "        #'solver': ['liblinear', 'saga'],\n",
    "        #'max_iter': [100, 200, 300],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"simple\",\"undersampling\",\"smote\"]\n",
    "percents = [\"1\",\"3\",\"5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING : GRID SEARCH - OPTIMISATION F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import f1_score \n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import pickle\n",
    "\n",
    "\n",
    "# for m in methods:\n",
    "#     if m == \"simple\":\n",
    "#         df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train.csv\")\n",
    "#         df_train = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "#         X_train = df_train.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "#         X_train = standardize_dataframe(X_train) # on standardise les données\n",
    "#         y_train = df_train[\"FlagImpaye\"]\n",
    "\n",
    "#         # Boucle sur chaque modèle\n",
    "#         for model_name, model in models.items():\n",
    "#             print(f\"\\nTraining {model_name} for method {m}\")\n",
    "\n",
    "#             # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "#             param_grid = param_grids[model_name]\n",
    "#             print(param_grid)\n",
    "\n",
    "#             f1_scorer = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "#             # Utiliser TimeSeriesSplit pour la validation croisée\n",
    "#             tscv = TimeSeriesSplit(n_splits=4)\n",
    "    \n",
    "#             # Créer la grille de recherche avec votre fonction personnalisée comme mesure d'évaluation\n",
    "#             grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=tscv, n_jobs=-1)\n",
    "\n",
    "#             # Effectuer la recherche de grille\n",
    "#             grid_search.fit(X_train, y_train)\n",
    "\n",
    "    \n",
    "#             # Afficher les résultats\n",
    "#             print(f\"\\n     Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "#             print(f\"     Meilleur f1 score pour {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "#             # Sauvegarder le meilleur modèle si nécessaire\n",
    "#             best_model = grid_search.best_estimator_\n",
    "#             filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "#             pickle.dump(best_model, open(filename, \"wb\"))\n",
    "\n",
    "#     else:\n",
    "#         for p in percents:\n",
    "#             df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train_\"+p+\"_percent.csv\")\n",
    "#             df_train = df_train.sort_values(by=\"Heure\")\n",
    "        \n",
    "#             X_train = df_train.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "\n",
    "#             X_train = standardize_dataframe(X_train) # on standardise les données\n",
    "#             y_train = df_train[\"FlagImpaye\"]\n",
    "\n",
    "#             # Boucle sur chaque modèle\n",
    "#             for model_name, model in models.items():\n",
    "#                 print(f\"\\nTraining {model_name} for method {m} and {p} % of frauds\")\n",
    "\n",
    "#                 # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "#                 param_grid = param_grids[model_name]\n",
    "#                 print(param_grid)\n",
    "\n",
    "#                 f1_scorer = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "#                 # Utiliser TimeSeriesSplit pour la validation croisée\n",
    "#                 tscv = TimeSeriesSplit(n_splits=4)\n",
    "                \n",
    "                \n",
    "#                 # Créer la grille de recherche avec votre fonction personnalisée comme mesure d'évaluation\n",
    "#                 grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=tscv, n_jobs=-1)\n",
    "\n",
    "#                 # Effectuer la recherche de grille\n",
    "#                 grid_search.fit(X_train, y_train)\n",
    "\n",
    "                \n",
    "#                 # Afficher les résultats\n",
    "#                 print(f\"\\n     Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "#                 print(f\"     Meilleur f1 score pour {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "#                 # Sauvegarder le meilleur modèle si nécessaire\n",
    "#                 best_model = grid_search.best_estimator_\n",
    "#                 filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "#                 pickle.dump(best_model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH MARGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Support_Vector_Machine for method simple\n",
      "{'C': 0.1, 'kernel': 'linear', 'class_weight': None, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train.csv\")\n",
    "        for model_name, model in models.items():\n",
    "                print(f\"\\nTraining {model_name} for method {m}\")\n",
    "\n",
    "                # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "                param_grid = param_grids[model_name]\n",
    "\n",
    "                # Initialiser les variables pour stocker les meilleurs paramètres et le meilleur score\n",
    "                best_params = None\n",
    "                best_score = 0\n",
    "\n",
    "                # Effectuer une recherche par grille manuelle\n",
    "                for params in product(*param_grid.values()):\n",
    "                    param_dict = dict(zip(param_grid.keys(), params))\n",
    "                    print(param_dict)\n",
    "\n",
    "                    # Initialiser le modèle avec les paramètres actuels\n",
    "                    clf = model.set_params(**param_dict)\n",
    "\n",
    "                    # Effectuer une validation croisée avec TimeSeriesSplit\n",
    "                    tscv = TimeSeriesSplit(n_splits=4)\n",
    "                    scores = []\n",
    "                    \n",
    "                    df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "                    X_train = df_train_sorted.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "                    y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "                    \n",
    "                    for train_index, val_index in tscv.split(X_train):\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                        # train :\n",
    "                        X_train_fold = standardize_dataframe(X_train_fold) # on standardise les données*\n",
    "                        \n",
    "                        #test :\n",
    "                        montants = X_val_fold[\"Montant\"]\n",
    "                        montants = montants.values\n",
    "                        X_val_fold = standardize_dataframe(X_val_fold) # on standardise les données*\n",
    "\n",
    "                        # Entraîner le modèle\n",
    "                        clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                        # Prédire sur l'ensemble de validation\n",
    "                        y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "                        y_val_fold = y_val_fold.values\n",
    "                        y_val_pred = y_val_pred.tolist()     \n",
    "                        y_val_fold = y_val_fold.tolist()\n",
    "                        montants = montants.tolist()\n",
    "\n",
    "                        \n",
    "                        score_marge = custom_score(y_val_fold, y_val_pred,montants)        \n",
    "                        print(\"  marge : \",score_marge)\n",
    "                        scores.append(score_marge)\n",
    "                    \n",
    "                    # Calculer le score moyen sur les plis\n",
    "                    avg_score = np.mean(scores)\n",
    "                    print(\"   moyenne : \",avg_score)\n",
    "\n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = param_dict\n",
    "                        best_model = clf\n",
    "\n",
    "\n",
    "                # Afficher les résultats\n",
    "                print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "                #print(f\"     Best marge pour {model_name}: {best_score}\")\n",
    "\n",
    "                # Sauvegarder le meilleur modèle\n",
    "                filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "                pickle.dump(best_model, open(filename, \"wb\"))\n",
    "    else:\n",
    "        for p in percents:\n",
    "            df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train_\"+p+\"_percent.csv\")\n",
    "\n",
    "            # Boucle sur chaque modèle\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"\\nTraining {model_name} for method {m} and {p} % of frauds\")\n",
    "\n",
    "                # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "                param_grid = param_grids[model_name]\n",
    "\n",
    "                # Initialiser les variables pour stocker les meilleurs paramètres et le meilleur score\n",
    "                best_params = None\n",
    "                best_score = 0\n",
    "\n",
    "                # Effectuer une recherche par grille manuelle\n",
    "                for params in product(*param_grid.values()):\n",
    "                    param_dict = dict(zip(param_grid.keys(), params))\n",
    "                    print(param_dict)\n",
    "\n",
    "                    # Initialiser le modèle avec les paramètres actuels\n",
    "                    clf = model.set_params(**param_dict)\n",
    "\n",
    "                    # Effectuer une validation croisée avec TimeSeriesSplit\n",
    "                    tscv = TimeSeriesSplit(n_splits=4)\n",
    "                    scores = []\n",
    "                    \n",
    "                    df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "                    X_train = df_train_sorted.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "                    y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "                    \n",
    "                    for train_index, val_index in tscv.split(X_train):\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                        # train :\n",
    "                        X_train_fold = standardize_dataframe(X_train_fold) # on standardise les données*\n",
    "                        \n",
    "                        #test :\n",
    "                        montants = X_val_fold[\"Montant\"]\n",
    "                        montants = montants.values\n",
    "                        X_val_fold = standardize_dataframe(X_val_fold) # on standardise les données*\n",
    "\n",
    "                        # Entraîner le modèle\n",
    "                        clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                        # Prédire sur l'ensemble de validation\n",
    "                        y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "                        y_val_fold = y_val_fold.values\n",
    "                        y_val_pred = y_val_pred.tolist()     \n",
    "                        y_val_fold = y_val_fold.tolist()\n",
    "                        montants = montants.tolist()\n",
    "\n",
    "                        \n",
    "                        score_marge = custom_score(y_val_fold, y_val_pred,montants)        \n",
    "                        print(\"  marge : \",score_marge)\n",
    "                        scores.append(score_marge)\n",
    "                    \n",
    "                    # Calculer le score moyen sur les plis\n",
    "                    avg_score = np.mean(scores)\n",
    "                    print(\"   moyenne : \",avg_score)\n",
    "\n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = param_dict\n",
    "                        best_model = clf\n",
    "\n",
    "\n",
    "                # Afficher les résultats\n",
    "                print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "                #print(f\"     Best marge pour {model_name}: {best_score}\")\n",
    "\n",
    "                # Sauvegarder le meilleur modèle\n",
    "                filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "                pickle.dump(best_model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "loaded_models = {method: {percent: None for percent in percents} for method in methods}\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        loaded_models[m] = {}\n",
    "        for model_name in models.keys():\n",
    "            filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "            with open(filename, 'rb') as file:\n",
    "                loaded_models[m][model_name] = pickle.load(file)\n",
    "    else:\n",
    "        for p in percents:\n",
    "            loaded_models[m][p] = {}\n",
    "            for model_name in models.keys():\n",
    "                filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "                with open(filename, 'rb') as file:\n",
    "                    loaded_models[m][p][model_name] = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING WITH BEST MARGE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random_Forest\n",
      "RandomForestClassifier(class_weight={0: 1.0, 1: 20.0}, max_depth=10,\n",
      "                       random_state=42)\n",
      "\n",
      " #### Testing Random_Forest for method simple ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.09256833464326261\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 2002581.73\n",
      "     Montant gagné avec le modèle : 60730.04000000004\n",
      "xgb_model\n",
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=100, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=42, ...)\n",
      "\n",
      " #### Testing xgb_model for method simple ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.053274608585477695\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 2014809.57\n",
      "     Montant gagné avec le modèle : 72957.88000000012\n",
      "\n",
      " #### Testing Random_Forest for method undersampling and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.010065706696490984\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1949305.8\n",
      "     Montant gagné avec le modèle : 7454.110000000102\n",
      "\n",
      " #### Testing xgb_model for method undersampling and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.029808374733853792\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 1968845.38\n",
      "     Montant gagné avec le modèle : 26993.689999999944\n",
      "\n",
      " #### Testing Random_Forest for method undersampling and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.02845167026546425\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1984555.61\n",
      "     Montant gagné avec le modèle : 42703.92000000016\n",
      "\n",
      " #### Testing xgb_model for method undersampling and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.07917645433190547\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 2046338.05\n",
      "     Montant gagné avec le modèle : 104486.3600000001\n",
      "\n",
      " #### Testing Random_Forest for method undersampling and 5 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.027379325581585647\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1958502.65\n",
      "     Montant gagné avec le modèle : 16650.959999999963\n",
      "\n",
      " #### Testing xgb_model for method undersampling and 5 % of frauds ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.03852506861734497\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 2025025.42\n",
      "     Montant gagné avec le modèle : 83173.72999999998\n",
      "\n",
      " #### Testing Random_Forest for method smote and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.0030358227079538558\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1942056.2\n",
      "     Montant gagné avec le modèle : 204.5100000000093\n",
      "\n",
      " #### Testing xgb_model for method smote and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.020568688483216958\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 1878086.19\n",
      "     Montant gagné avec le modèle : -63765.5\n",
      "\n",
      " #### Testing Random_Forest for method smote and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.006263048016701462\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1945739.11\n",
      "     Montant gagné avec le modèle : 3887.4200000001583\n",
      "\n",
      " #### Testing xgb_model for method smote and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.018605321987452222\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 1870332.09\n",
      "     Montant gagné avec le modèle : -71519.59999999986\n",
      "\n",
      " #### Testing Random_Forest for method smote and 5 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.003938200545289306\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1944881.13\n",
      "     Montant gagné avec le modèle : 3029.439999999944\n",
      "\n",
      " #### Testing xgb_model for method smote and 5 % of frauds ####\n",
      "\n",
      "     F1 score de  xgb_model  sur l'ensemble de test : 0.026839119259969915\n",
      "     Marge de  xgb_model  sur l'ensemble de test : 1946809.03\n",
      "     Montant gagné avec le modèle : 4957.340000000084\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "index_modified = []\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        index_modified.append(m)\n",
    "    else:\n",
    "        for p in percents:\n",
    "            index_modified.append(m+\"_\"+p)\n",
    "  \n",
    "\n",
    "f1_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "marge_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "montant_gagne_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "\n",
    "df_test = pd.read_csv(\"../data/simple/dataframe_test.csv\")\n",
    "X_test = df_test.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "montants = X_test[\"Montant\"]\n",
    "montants = montants.values\n",
    "montants = montants.tolist()\n",
    "\n",
    "X_test = standardize_dataframe(X_test) # on standardise les données test\n",
    "\n",
    "y_test = df_test[\"FlagImpaye\"]\n",
    "y_test = y_test.values\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        for model_name, model in loaded_models.get(m, {}).items():\n",
    "            print(f\"\\n #### Testing {model_name} for method {m} ####\")\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = y_pred.tolist()\n",
    "\n",
    "            nom_ligne = m\n",
    "\n",
    "            # f1 score\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            f1_df.loc[nom_ligne, model_name] = f1\n",
    "\n",
    "            # marge\n",
    "            score_marge = custom_score(y_test, y_pred,montants)   \n",
    "            score_marge = round(score_marge,2)\n",
    "            marge_df.loc[nom_ligne, model_name] = score_marge\n",
    "\n",
    "         \n",
    "            marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "            montant_gagne = round(score_marge - marge_laisse_passer_tout_le_monde,2)\n",
    "            montant_gagne_df.at[nom_ligne ,model_name] = montant_gagne\n",
    "           \n",
    "\n",
    "            # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            # disp = ConfusionMatrixDisplay(conf_matrix, display_labels=[False, True])\n",
    "            # disp.plot()\n",
    "            # plt.show()\n",
    "\n",
    "            print(\"\\n     F1 score de \",model_name,\" sur l'ensemble de test :\", f1)\n",
    "            print(\"     Marge de \",model_name,\" sur l'ensemble de test :\", score_marge)\n",
    "            print(\"     Montant gagné avec le modèle :\", montant_gagne)\n",
    "            \n",
    "    else:\n",
    "        for p, model_dic in loaded_models.get(m, {}).items():\n",
    "            for model_name, model in model_dic.items():\n",
    "                print(f\"\\n #### Testing {model_name} for method {m} and {p} % of frauds ####\")\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred = y_pred.tolist()\n",
    "\n",
    "                nom_ligne = m+\"_\"+p\n",
    "\n",
    "                # f1 score\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                f1_df.loc[nom_ligne, model_name] = f1\n",
    "\n",
    "                # marge\n",
    "                score_marge = custom_score(y_test, y_pred,montants)   \n",
    "                score_marge = round(score_marge,2)\n",
    "                marge_df.loc[nom_ligne, model_name] = score_marge\n",
    "\n",
    "                marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "                montant_gagne = round(score_marge - marge_laisse_passer_tout_le_monde,2)\n",
    "                montant_gagne_df.at[nom_ligne ,model_name] = montant_gagne\n",
    "        \n",
    "                # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                # disp = ConfusionMatrixDisplay(conf_matrix, display_labels=[False, True])\n",
    "                # disp.plot()\n",
    "                # plt.show()\n",
    "\n",
    "                print(\"\\n     F1 score de \",model_name,\" sur l'ensemble de test :\", f1)\n",
    "                print(\"     Marge de \",model_name,\" sur l'ensemble de test :\", score_marge)\n",
    "                print(\"     Montant gagné avec le modèle :\", montant_gagne)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marge Parfait :  2294459.36\n",
      "Marge si on laisse passer tout le monde :  1941851.69\n"
     ]
    }
   ],
   "source": [
    "marge_parfaite = round(custom_score(y_test, y_test,montants),2)\n",
    "marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "\n",
    "print(\"Marge Parfaite : \",marge_parfaite)\n",
    "print(\"Marge si on laisse passer tout le monde : \",marge_laisse_passer_tout_le_monde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>xgb_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>2002581.73</td>\n",
       "      <td>2014809.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>1949305.8</td>\n",
       "      <td>1968845.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>1984555.61</td>\n",
       "      <td>2046338.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_5</th>\n",
       "      <td>1958502.65</td>\n",
       "      <td>2025025.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_1</th>\n",
       "      <td>1942056.2</td>\n",
       "      <td>1878086.19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_3</th>\n",
       "      <td>1945739.11</td>\n",
       "      <td>1870332.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_5</th>\n",
       "      <td>1944881.13</td>\n",
       "      <td>1946809.03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random_Forest   xgb_model\n",
       "simple             2002581.73  2014809.57\n",
       "undersampling_1     1949305.8  1968845.38\n",
       "undersampling_3    1984555.61  2046338.05\n",
       "undersampling_5    1958502.65  2025025.42\n",
       "smote_1             1942056.2  1878086.19\n",
       "smote_3            1945739.11  1870332.09\n",
       "smote_5            1944881.13  1946809.03"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marge_df.to_csv('../data/marge.csv')\n",
    "marge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montant gagné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>xgb_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>60730.04</td>\n",
       "      <td>72957.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>7454.11</td>\n",
       "      <td>26993.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>42703.92</td>\n",
       "      <td>104486.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_5</th>\n",
       "      <td>16650.96</td>\n",
       "      <td>83173.73</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_1</th>\n",
       "      <td>204.51</td>\n",
       "      <td>-63765.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_3</th>\n",
       "      <td>3887.42</td>\n",
       "      <td>-71519.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_5</th>\n",
       "      <td>3029.44</td>\n",
       "      <td>4957.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random_Forest  xgb_model\n",
       "simple               60730.04   72957.88\n",
       "undersampling_1       7454.11   26993.69\n",
       "undersampling_3      42703.92  104486.36\n",
       "undersampling_5      16650.96   83173.73\n",
       "smote_1                204.51   -63765.5\n",
       "smote_3               3887.42   -71519.6\n",
       "smote_5               3029.44    4957.34"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montant_gagne_df.to_csv('../data/montant_gagne.csv')\n",
    "montant_gagne_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>xgb_model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.092568</td>\n",
       "      <td>0.053275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>0.010066</td>\n",
       "      <td>0.029808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>0.028452</td>\n",
       "      <td>0.079176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_5</th>\n",
       "      <td>0.027379</td>\n",
       "      <td>0.038525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_1</th>\n",
       "      <td>0.003036</td>\n",
       "      <td>0.020569</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_3</th>\n",
       "      <td>0.006263</td>\n",
       "      <td>0.018605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_5</th>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.026839</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random_Forest xgb_model\n",
       "simple               0.092568  0.053275\n",
       "undersampling_1      0.010066  0.029808\n",
       "undersampling_3      0.028452  0.079176\n",
       "undersampling_5      0.027379  0.038525\n",
       "smote_1              0.003036  0.020569\n",
       "smote_3              0.006263  0.018605\n",
       "smote_5              0.003938  0.026839"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_df.to_csv('../data/f1_score.csv')\n",
    "f1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
