{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARDISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_dataframe(dataframe):\n",
    "    # Sélectionner uniquement les colonnes numériques\n",
    "    numeric_cols = dataframe.select_dtypes(include=['float64', 'int64','int32']).columns\n",
    "    # Copier le DataFrame pour éviter de modifier l'original\n",
    "    standardized_df = dataframe.copy()\n",
    "    # Standardiser les colonnes numériques\n",
    "    scaler = StandardScaler()\n",
    "    standardized_df[numeric_cols] = scaler.fit_transform(dataframe[numeric_cols])\n",
    "    \n",
    "    return standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximisation_marge(montant,status):\n",
    "    taux_marge = 0.05\n",
    "    if status == \"TP\":\n",
    "        res = 0 # le fraudeur est détecté\n",
    "    elif status == \"TN\":\n",
    "        res = taux_marge * montant # un client honnête est accepté\n",
    "    elif status == \"FP\":\n",
    "        res = 0.7*taux_marge * montant # un client honnête est bloqué     \n",
    "    elif status == \"FN\": # un fraudeur est accepté \n",
    "        if montant <= 20:\n",
    "            res = 0\n",
    "        elif montant <= 50:\n",
    "            res = -0.2 * montant\n",
    "        elif montant <= 100:\n",
    "            res = -0.3 * montant\n",
    "        elif montant <= 200:\n",
    "            res = -0.5 * montant\n",
    "        else:\n",
    "            res = -0.8 * montant\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATION - SCORER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(y_true, y_pred, montants):\n",
    "    total_marge = 0\n",
    "    inde = 0\n",
    "    for i in range(len(montants)):\n",
    "        status = \"\"\n",
    "        if y_true[inde] == 1 and y_pred[inde] == 1:\n",
    "            status = \"TP\"\n",
    "        elif y_true[inde] == 0 and y_pred[inde] == 0:\n",
    "            status = \"TN\"\n",
    "        elif y_true[inde] == 0 and y_pred[inde] == 1:\n",
    "            status = \"FP\"\n",
    "        elif y_true[inde] == 1 and y_pred[inde] == 0:\n",
    "            status = \"FN\" \n",
    "\n",
    "        total_marge += maximisation_marge(montants[inde], status)\n",
    "        inde += 1\n",
    "    \n",
    "    return total_marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATION - CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "class_weights = {0: 1.0, 1: 20.0}\n",
    "\n",
    "models = {\n",
    "    'Random_Forest': RandomForestClassifier(), # LUI\n",
    "    'xgb_model': xgb.XGBClassifier(), # LUI\n",
    "    #'Gradient_Boosting': GradientBoostingClassifier(),\n",
    "    #'K_Nearest_Neighbors': KNeighborsClassifier(),\n",
    "    'Support_Vector_Machine': SVC(), # LUI\n",
    "    #'Neural_Network': MLPClassifier(),\n",
    "    'Linear_Discriminant_Analysis': LinearDiscriminantAnalysis() # LUI\n",
    "    #'Logistic_Regression': LogisticRegression()\n",
    "}\n",
    "\n",
    "models = {\n",
    "    'Random_Forest': RandomForestClassifier(), # LUI\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Random_Forest': {\n",
    "        'criterion': ['gini'],\n",
    "        'n_estimators': [5,100],\n",
    "        'max_depth': [None, 10],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'random_state': [42]\n",
    "        },\n",
    "    'xgb_model': {\n",
    "        'objective': ['binary:logistic'],\n",
    "        'n_estimators': [5,100],\n",
    "        #'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [None,3,10],\n",
    "        'subsample': [0.5,1.0],\n",
    "        #'colsample_bytree': [0.8, 1.0],\n",
    "        #'gamma': [0, 0.1, 0.2],\n",
    "        #'min_child_weight': [1, 5, 10],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'Gradient_Boosting': {\n",
    "        #'n_estimators': [50, 100, 200],\n",
    "        #'learning_rate': [0.01, 0.1, 0.2],\n",
    "        #'max_depth': [3, 5, 7],\n",
    "        #'subsample': [0.8, 1.0],\n",
    "        #'min_samples_split': [2, 5, 10],\n",
    "        #'min_samples_leaf': [1, 2, 4],\n",
    "        #'max_features': [None, 'sqrt', 'log2'],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'K_Nearest_Neighbors': {\n",
    "        'n_neighbors': [3,10],\n",
    "        #'weights': ['uniform', 'distance'],\n",
    "        #'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        #'leaf_size': [20, 30, 40],\n",
    "        #'p': [1, 2]\n",
    "    },\n",
    "    'Support_Vector_Machine': {\n",
    "        'C': [0.1, 1.0, 5.0],\n",
    "        'kernel': ['linear','rbf'],\n",
    "        #'degree': [2, 3, 4],\n",
    "        #'gamma': ['scale', 'auto'],\n",
    "        'class_weight': [None,class_weights],\n",
    "        'random_state': [42]    \n",
    "       },\n",
    "   'Neural_Network': {\n",
    "        #'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "        #'activation': ['relu', 'tanh', 'logistic'],\n",
    "        #'solver': ['sgd', 'adam'],\n",
    "        #'alpha': [0.0001, 0.001, 0.01],\n",
    "        #'learning_rate': ['constant', 'invscaling', 'adaptive'],\n",
    "        #'max_iter': [100, 200, 300],\n",
    "        #'early_stopping': [True, False],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'Linear_Discriminant_Analysis': {\n",
    "        'solver': ['svd', 'lsqr'],\n",
    "        'shrinkage': [None, 'auto']\n",
    "        #'n_components': [None, 1, 2, 3]\n",
    "    },\n",
    "     'Logistic_Regression': {\n",
    "        #'penalty': ['l1', 'l2'],\n",
    "        #'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "        #'fit_intercept': [True, False],\n",
    "        #'class_weight': [None, 'balanced'],\n",
    "        #'solver': ['liblinear', 'saga'],\n",
    "        #'max_iter': [100, 200, 300],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    'Random_Forest': {\n",
    "        'criterion': ['gini'],\n",
    "        'n_estimators': [1],\n",
    "        'max_depth': [1],\n",
    "        'class_weight' : [None],\n",
    "        'random_state': [42]\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"simple\",\"undersampling\",\"smote\"]\n",
    "percents = [\"1\",\"3\",\"5\"]\n",
    "\n",
    "methods = [\"simple\",\"undersampling\"]\n",
    "percents = [\"1\",\"3\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING : GRID SEARCH - OPTIMISATION F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.metrics import make_scorer\n",
    "# from sklearn.metrics import f1_score \n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import pickle\n",
    "\n",
    "\n",
    "# for m in methods:\n",
    "#     if m == \"simple\":\n",
    "#         df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train.csv\")\n",
    "#         df_train = df_train.sort_values(by=\"Heure\")\n",
    "#         for model_name, model in models.items():\n",
    "#                 print(f\"\\n Training {model_name} for method {m}\")\n",
    "\n",
    "#                 X_train = df_train.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "#                 X_train = standardize_dataframe(X_train) # on standardise les données\n",
    "#                 y_train = df_train[\"FlagImpaye\"]\n",
    "\n",
    "#                 # Boucle sur chaque modèle\n",
    "#                 for model_name, model in models.items():\n",
    "#                     print(f\"\\nTraining {model_name} for method {m}\")\n",
    "\n",
    "#                     # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "#                     param_grid = param_grids[model_name]\n",
    "#                     print(param_grid)\n",
    "\n",
    "#                     f1_scorer = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "#                     # Utiliser TimeSeriesSplit pour la validation croisée\n",
    "#                     tscv = TimeSeriesSplit(n_splits=4)\n",
    "                    \n",
    "#                     # Créer la grille de recherche avec votre fonction personnalisée comme mesure d'évaluation\n",
    "#                     grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=tscv, n_jobs=-1)\n",
    "\n",
    "#                     # Effectuer la recherche de grille\n",
    "#                     grid_search.fit(X_train, y_train)\n",
    "\n",
    "                    \n",
    "#                     # Afficher les résultats\n",
    "#                     print(f\"\\n     Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "#                     print(f\"     Meilleur f1 score pour {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "#                     # Sauvegarder le meilleur modèle si nécessaire\n",
    "#                     best_model = grid_search.best_estimator_\n",
    "#                     filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "#                     pickle.dump(best_model, open(filename, \"wb\"))\n",
    "#     else:\n",
    "#         for p in percents:\n",
    "#             df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train_\"+p+\"_percent.csv\")\n",
    "#             df_train = df_train.sort_values(by=\"Heure\")\n",
    "        \n",
    "#             X_train = df_train.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "\n",
    "#             X_train = standardize_dataframe(X_train) # on standardise les données\n",
    "#             y_train = df_train[\"FlagImpaye\"]\n",
    "\n",
    "#             # Boucle sur chaque modèle\n",
    "#             for model_name, model in models.items():\n",
    "#                 print(f\"\\nTraining {model_name} for method {m} and {p} % of frauds\")\n",
    "\n",
    "#                 # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "#                 param_grid = param_grids[model_name]\n",
    "#                 print(param_grid)\n",
    "\n",
    "#                 f1_scorer = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "#                 # Utiliser TimeSeriesSplit pour la validation croisée\n",
    "#                 tscv = TimeSeriesSplit(n_splits=4)\n",
    "                \n",
    "                \n",
    "#                 # Créer la grille de recherche avec votre fonction personnalisée comme mesure d'évaluation\n",
    "#                 grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=tscv, n_jobs=-1)\n",
    "\n",
    "#                 # Effectuer la recherche de grille\n",
    "#                 grid_search.fit(X_train, y_train)\n",
    "\n",
    "                \n",
    "#                 # Afficher les résultats\n",
    "#                 print(f\"\\n     Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "#                 print(f\"     Meilleur f1 score pour {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "#                 # Sauvegarder le meilleur modèle si nécessaire\n",
    "#                 best_model = grid_search.best_estimator_\n",
    "#                 filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "#                 pickle.dump(best_model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRID SEARCH MARGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random_Forest for method simple\n",
      "{'criterion': 'gini', 'n_estimators': 1, 'max_depth': 1, 'class_weight': None, 'random_state': 42}\n",
      "  marge :  2158817.8545003147\n",
      "  marge :  2092674.8860003653\n",
      "  marge :  2065021.7640003532\n",
      "  marge :  2035562.826000033\n",
      "   moyenne :  2088019.3326252666\n",
      "\n",
      "     Best parameters pour Random_Forest: {'criterion': 'gini', 'n_estimators': 1, 'max_depth': 1, 'class_weight': None, 'random_state': 42}\n",
      "\n",
      "Training Random_Forest for method undersampling and 1 % of frauds\n",
      "{'criterion': 'gini', 'n_estimators': 1, 'max_depth': 1, 'class_weight': None, 'random_state': 42}\n",
      "  marge :  972471.5477999275\n",
      "  marge :  932688.6466998949\n",
      "  marge :  970552.605149914\n",
      "  marge :  936529.6160999556\n",
      "   moyenne :  953060.603937423\n",
      "\n",
      "     Best parameters pour Random_Forest: {'criterion': 'gini', 'n_estimators': 1, 'max_depth': 1, 'class_weight': None, 'random_state': 42}\n",
      "\n",
      "Training Random_Forest for method undersampling and 3 % of frauds\n",
      "{'criterion': 'gini', 'n_estimators': 1, 'max_depth': 1, 'class_weight': None, 'random_state': 42}\n",
      "  marge :  246717.2383499901\n",
      "  marge :  227673.2418499882\n",
      "  marge :  265871.9049499861\n",
      "  marge :  201282.76929999446\n",
      "   moyenne :  235386.2886124897\n",
      "\n",
      "     Best parameters pour Random_Forest: {'criterion': 'gini', 'n_estimators': 1, 'max_depth': 1, 'class_weight': None, 'random_state': 42}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train.csv\")\n",
    "        for model_name, model in models.items():\n",
    "                print(f\"\\nTraining {model_name} for method {m}\")\n",
    "\n",
    "                # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "                param_grid = param_grids[model_name]\n",
    "\n",
    "                # Initialiser les variables pour stocker les meilleurs paramètres et le meilleur score\n",
    "                best_params = None\n",
    "                best_score = 0\n",
    "\n",
    "                # Effectuer une recherche par grille manuelle\n",
    "                for params in product(*param_grid.values()):\n",
    "                    param_dict = dict(zip(param_grid.keys(), params))\n",
    "                    print(param_dict)\n",
    "\n",
    "                    # Initialiser le modèle avec les paramètres actuels\n",
    "                    clf = model.set_params(**param_dict)\n",
    "\n",
    "                    # Effectuer une validation croisée avec TimeSeriesSplit\n",
    "                    tscv = TimeSeriesSplit(n_splits=4)\n",
    "                    scores = []\n",
    "                    \n",
    "                    df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "                    X_train = df_train_sorted.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "                    y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "                    \n",
    "                    for train_index, val_index in tscv.split(X_train):\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                        # train :\n",
    "                        X_train_fold = standardize_dataframe(X_train_fold) # on standardise les données*\n",
    "                        \n",
    "                        #test :\n",
    "                        montants = X_val_fold[\"Montant\"]\n",
    "                        montants = montants.values\n",
    "                        X_val_fold = standardize_dataframe(X_val_fold) # on standardise les données*\n",
    "\n",
    "                        # Entraîner le modèle\n",
    "                        clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                        # Prédire sur l'ensemble de validation\n",
    "                        y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "                        y_val_fold = y_val_fold.values\n",
    "                        y_val_pred = y_val_pred.tolist()     \n",
    "                        y_val_fold = y_val_fold.tolist()\n",
    "                        montants = montants.tolist()\n",
    "\n",
    "                        \n",
    "                        score_marge = custom_score(y_val_fold, y_val_pred,montants)        \n",
    "                        print(\"  marge : \",score_marge)\n",
    "                        scores.append(score_marge)\n",
    "                    \n",
    "                    # Calculer le score moyen sur les plis\n",
    "                    avg_score = np.mean(scores)\n",
    "                    print(\"   moyenne : \",avg_score)\n",
    "\n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = param_dict\n",
    "                        best_model = clf\n",
    "\n",
    "\n",
    "                # Afficher les résultats\n",
    "                print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "                #print(f\"     Best marge pour {model_name}: {best_score}\")\n",
    "\n",
    "                # Sauvegarder le meilleur modèle\n",
    "                filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "                pickle.dump(best_model, open(filename, \"wb\"))\n",
    "    else:\n",
    "        for p in percents:\n",
    "            df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train_\"+p+\"_percent.csv\")\n",
    "\n",
    "            # Boucle sur chaque modèle\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"\\nTraining {model_name} for method {m} and {p} % of frauds\")\n",
    "\n",
    "                # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "                param_grid = param_grids[model_name]\n",
    "\n",
    "                # Initialiser les variables pour stocker les meilleurs paramètres et le meilleur score\n",
    "                best_params = None\n",
    "                best_score = 0\n",
    "\n",
    "                # Effectuer une recherche par grille manuelle\n",
    "                for params in product(*param_grid.values()):\n",
    "                    param_dict = dict(zip(param_grid.keys(), params))\n",
    "                    print(param_dict)\n",
    "\n",
    "                    # Initialiser le modèle avec les paramètres actuels\n",
    "                    clf = model.set_params(**param_dict)\n",
    "\n",
    "                    # Effectuer une validation croisée avec TimeSeriesSplit\n",
    "                    tscv = TimeSeriesSplit(n_splits=4)\n",
    "                    scores = []\n",
    "                    \n",
    "                    df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "                    X_train = df_train_sorted.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "                    y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "                    \n",
    "                    for train_index, val_index in tscv.split(X_train):\n",
    "                        X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "                        y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "                        # train :\n",
    "                        X_train_fold = standardize_dataframe(X_train_fold) # on standardise les données*\n",
    "                        \n",
    "                        #test :\n",
    "                        montants = X_val_fold[\"Montant\"]\n",
    "                        montants = montants.values\n",
    "                        X_val_fold = standardize_dataframe(X_val_fold) # on standardise les données*\n",
    "\n",
    "                        # Entraîner le modèle\n",
    "                        clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "                        # Prédire sur l'ensemble de validation\n",
    "                        y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "                        y_val_fold = y_val_fold.values\n",
    "                        y_val_pred = y_val_pred.tolist()     \n",
    "                        y_val_fold = y_val_fold.tolist()\n",
    "                        montants = montants.tolist()\n",
    "\n",
    "                        \n",
    "                        score_marge = custom_score(y_val_fold, y_val_pred,montants)        \n",
    "                        print(\"  marge : \",score_marge)\n",
    "                        scores.append(score_marge)\n",
    "                    \n",
    "                    # Calculer le score moyen sur les plis\n",
    "                    avg_score = np.mean(scores)\n",
    "                    print(\"   moyenne : \",avg_score)\n",
    "\n",
    "                    if avg_score > best_score:\n",
    "                        best_score = avg_score\n",
    "                        best_params = param_dict\n",
    "                        best_model = clf\n",
    "\n",
    "\n",
    "                # Afficher les résultats\n",
    "                print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "                #print(f\"     Best marge pour {model_name}: {best_score}\")\n",
    "\n",
    "                # Sauvegarder le meilleur modèle\n",
    "                filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "                pickle.dump(best_model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "methods = [\"simple\",\"undersampling\",\"smote\"]\n",
    "percents = [\"1\",\"3\",\"5\"]\n",
    "\n",
    "methods = [\"simple\",\"undersampling\"]\n",
    "percents = [\"1\",\"3\"]\n",
    "\n",
    "\n",
    "loaded_models = {method: {percent: None for percent in percents} for method in methods}\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        loaded_models[m] = {}\n",
    "        for model_name in models.keys():\n",
    "            filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "            with open(filename, 'rb') as file:\n",
    "                loaded_models[m][model_name] = pickle.load(file)\n",
    "    else:\n",
    "        for p in percents:\n",
    "            loaded_models[m][p] = {}\n",
    "            for model_name in models.keys():\n",
    "                filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "                with open(filename, 'rb') as file:\n",
    "                    loaded_models[m][p][model_name] = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING WITH BEST MARGE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'simple': {'Random_Forest': RandomForestClassifier(max_depth=1, n_estimators=1, random_state=42)}, 'undersampling': {'1': {'Random_Forest': RandomForestClassifier(max_depth=1, n_estimators=1, random_state=42)}, '3': {'Random_Forest': RandomForestClassifier(max_depth=1, n_estimators=1, random_state=42)}}}\n",
      "Random_Forest\n",
      "RandomForestClassifier(max_depth=1, n_estimators=1, random_state=42)\n",
      "\n",
      " #### Testing Random_Forest for method simple ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.0\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1941851.69\n",
      "     Montant gagné avec le modèle : 0.0\n",
      "\n",
      " #### Testing Random_Forest for method undersampling and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.010065706696490984\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1949305.8\n",
      "     Montant gagné avec le modèle : 7454.110000000102\n",
      "\n",
      " #### Testing Random_Forest for method undersampling and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  Random_Forest  sur l'ensemble de test : 0.02845167026546425\n",
      "     Marge de  Random_Forest  sur l'ensemble de test : 1984555.61\n",
      "     Montant gagné avec le modèle : 42703.92000000016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "index_modified = []\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        index_modified.append(m)\n",
    "    else:\n",
    "        for p in percents:\n",
    "            index_modified.append(m+\"_\"+p)\n",
    "  \n",
    "\n",
    "f1_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "marge_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "\n",
    "\n",
    "df_test = pd.read_csv(\"../data/simple/dataframe_test.csv\")\n",
    "X_test = df_test.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "montants = X_test[\"Montant\"]\n",
    "montants = montants.values\n",
    "montants = montants.tolist()\n",
    "\n",
    "X_test = standardize_dataframe(X_test) # on standardise les données test\n",
    "\n",
    "y_test = df_test[\"FlagImpaye\"]\n",
    "y_test = y_test.values\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "print(loaded_models)\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        for model_name, model in loaded_models.get(m, {}).items():\n",
    "            print(model_name)\n",
    "            print(model)\n",
    "            print(f\"\\n #### Testing {model_name} for method {m} ####\")\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = y_pred.tolist()\n",
    "\n",
    "            nom_ligne = m\n",
    "\n",
    "            # f1 score\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            f1_df.loc[nom_ligne, model_name] = f1\n",
    "\n",
    "            # marge\n",
    "            score_marge = custom_score(y_test, y_pred,montants)   \n",
    "            score_marge = round(score_marge,2)\n",
    "            marge_df.loc[nom_ligne, model_name] = score_marge\n",
    "\n",
    "         \n",
    "            marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "            marge_df.at[nom_ligne ,\"!_Montant_Gagné_!\"] = score_marge - marge_laisse_passer_tout_le_monde\n",
    "           \n",
    "\n",
    "            # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            # disp = ConfusionMatrixDisplay(conf_matrix, display_labels=[False, True])\n",
    "            # disp.plot()\n",
    "            # plt.show()\n",
    "\n",
    "            print(\"\\n     F1 score de \",model_name,\" sur l'ensemble de test :\", f1)\n",
    "            print(\"     Marge de \",model_name,\" sur l'ensemble de test :\", score_marge)\n",
    "            print(\"     Montant gagné avec le modèle :\", score_marge - marge_laisse_passer_tout_le_monde)\n",
    "            \n",
    "    else:\n",
    "        for p, model_dic in loaded_models.get(m, {}).items():\n",
    "            for model_name, model in model_dic.items():\n",
    "                print(f\"\\n #### Testing {model_name} for method {m} and {p} % of frauds ####\")\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred = y_pred.tolist()\n",
    "\n",
    "                nom_ligne = m+\"_\"+p\n",
    "\n",
    "                # f1 score\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                f1_df.loc[nom_ligne, model_name] = f1\n",
    "\n",
    "                # marge\n",
    "                score_marge = custom_score(y_test, y_pred,montants)   \n",
    "                score_marge = round(score_marge,2)\n",
    "                marge_df.loc[nom_ligne, model_name] = score_marge\n",
    "    \n",
    "               \n",
    "                marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "                marge_df.at[nom_ligne ,\"!_Montant_Gagné_!\"] = score_marge - marge_laisse_passer_tout_le_monde\n",
    "      \n",
    "                # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                # disp = ConfusionMatrixDisplay(conf_matrix, display_labels=[False, True])\n",
    "                # disp.plot()\n",
    "                # plt.show()\n",
    "\n",
    "                print(\"\\n     F1 score de \",model_name,\" sur l'ensemble de test :\", f1)\n",
    "                print(\"     Marge de \",model_name,\" sur l'ensemble de test :\", score_marge)\n",
    "                print(\"     Montant gagné avec le modèle :\", score_marge - marge_laisse_passer_tout_le_monde)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marge Parfait :  2294459.36\n",
      "Marge tous honnete :  1941851.69\n"
     ]
    }
   ],
   "source": [
    "marge_parfaite = round(custom_score(y_test, y_test,montants),2)\n",
    "marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "\n",
    "print(\"Marge Parfait : \",marge_parfaite)\n",
    "print(\"Marge tous honnete : \",marge_laisse_passer_tout_le_monde)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Forest</th>\n",
       "      <th>!_Montant_Gagné_!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>1941851.69</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>1949305.8</td>\n",
       "      <td>7454.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>1984555.61</td>\n",
       "      <td>42703.92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random_Forest  !_Montant_Gagné_!\n",
       "simple             1941851.69               0.00\n",
       "undersampling_1     1949305.8            7454.11\n",
       "undersampling_3    1984555.61           42703.92"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marge_df.to_csv('../data/marge.csv')\n",
    "marge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Random_Forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>0.010066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>0.028452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Random_Forest\n",
       "simple                    0.0\n",
       "undersampling_1      0.010066\n",
       "undersampling_3      0.028452"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_df.to_csv('../data/f1_score.csv')\n",
    "f1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
