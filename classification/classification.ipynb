{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STANDARDISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def standardize_dataframe(dataframe):\n",
    "    # Sélectionner uniquement les colonnes numériques\n",
    "    numeric_cols = dataframe.select_dtypes(include=['float64', 'int64','int32']).columns\n",
    "    # Copier le DataFrame pour éviter de modifier l'original\n",
    "    standardized_df = dataframe.copy()\n",
    "    # Standardiser les colonnes numériques\n",
    "    scaler = StandardScaler()\n",
    "    standardized_df[numeric_cols] = scaler.fit_transform(dataframe[numeric_cols])\n",
    "    \n",
    "    return standardized_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximisation_marge(montant,status):\n",
    "    taux_marge = 0.05\n",
    "    if status == \"TP\":\n",
    "        res = 0 # le fraudeur est détecté\n",
    "    elif status == \"TN\":\n",
    "        res = taux_marge * montant # un client honnête est accepté\n",
    "    elif status == \"FP\":\n",
    "        res = 0.7*taux_marge * montant # un client honnête est bloqué     \n",
    "    elif status == \"FN\": # un fraudeur est accepté \n",
    "        if montant <= 20:\n",
    "            res = 0\n",
    "        elif montant <= 50:\n",
    "            res = -0.2 * montant\n",
    "        elif montant <= 100:\n",
    "            res = -0.3 * montant\n",
    "        elif montant <= 200:\n",
    "            res = -0.5 * montant\n",
    "        else:\n",
    "            res = -0.8 * montant\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATION - SCORER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_score(y_true, y_pred, montants):\n",
    "    total_marge = 0\n",
    "    inde = 0\n",
    "    for i in range(len(montants)):\n",
    "        status = \"\"\n",
    "        if y_true[inde] == 1 and y_pred[inde] == 1:\n",
    "            status = \"TP\"\n",
    "        elif y_true[inde] == 0 and y_pred[inde] == 0:\n",
    "            status = \"TN\"\n",
    "        elif y_true[inde] == 0 and y_pred[inde] == 1:\n",
    "            status = \"FP\"\n",
    "        elif y_true[inde] == 1 and y_pred[inde] == 0:\n",
    "            status = \"FN\" \n",
    "\n",
    "        total_marge += maximisation_marge(montants[inde], status)\n",
    "        inde += 1\n",
    "    \n",
    "    return total_marge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INITIALISATION - CLASSIFIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Linear_Discriminant_Analysis': LinearDiscriminantAnalysis(), 'Logistic_Regression': LogisticRegression()}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "class_weights = {0: 1.0, 1: 20.0}\n",
    "\n",
    "models = {\n",
    "    #'Random_Forest': RandomForestClassifier(), # LUI\n",
    "    #'xgb_model': xgb.XGBClassifier(), # LUI\n",
    "    'Linear_Discriminant_Analysis': LinearDiscriminantAnalysis(), # LUI\n",
    "    'Logistic_Regression': LogisticRegression() # LUI\n",
    "}\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grids = {\n",
    "    'Random_Forest': {\n",
    "        'criterion': ['gini'],\n",
    "        'n_estimators': [5,100],\n",
    "        'max_depth': [None, 10],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'random_state': [42]\n",
    "        },\n",
    "    'xgb_model': {\n",
    "        'objective': ['binary:logistic'],\n",
    "        'n_estimators': [5,100],\n",
    "        #'learning_rate': [0.01, 0.1, 0.2],\n",
    "        'max_depth': [None,3,10],\n",
    "        'subsample': [0.5,1.0],\n",
    "        #'colsample_bytree': [0.8, 1.0],\n",
    "        #'gamma': [0, 0.1, 0.2],\n",
    "        #'min_child_weight': [1, 5, 10],\n",
    "        'random_state': [42]\n",
    "    },\n",
    "    'Linear_Discriminant_Analysis': {\n",
    "        'solver': ['svd', 'lsqr'],\n",
    "        'priors' : [None,[0.05, 0.95]]\n",
    "        #'shrinkage': [None, 'auto']\n",
    "        #'n_components': [None, 1, 2, 3]\n",
    "    },\n",
    "     'Logistic_Regression': {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        #'fit_intercept': [True, False],\n",
    "        'solver': ['liblinear'],\n",
    "        #'max_iter': [100, 200, 300],\n",
    "        'class_weight' : [None, class_weights],\n",
    "        'random_state': [42]\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "methods = [\"simple\",\"undersampling\",\"smote\"]\n",
    "percents = [\"1\",\"3\",\"5\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING : GRID SEARCH - F1 SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import f1_score \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle\n",
    "\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train.csv\")\n",
    "        df_train = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "\n",
    "        X_train = df_train.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "        X_train = standardize_dataframe(X_train) # on standardise les données\n",
    "        y_train = df_train[\"FlagImpaye\"]\n",
    "\n",
    "        # Boucle sur chaque modèle\n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\nTraining {model_name} for method {m}\")\n",
    "\n",
    "            # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "            param_grid = param_grids[model_name]\n",
    "            print(param_grid)\n",
    "\n",
    "            f1_scorer = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "            # Utiliser TimeSeriesSplit pour la validation croisée\n",
    "            tscv = TimeSeriesSplit(n_splits=4)\n",
    "            \n",
    "            # Créer la grille de recherche avec votre fonction personnalisée comme mesure d'évaluation\n",
    "            grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=tscv, n_jobs=-1)\n",
    "\n",
    "            # Effectuer la recherche de grille\n",
    "            grid_search.fit(X_train, y_train)\n",
    "\n",
    "            \n",
    "            # Afficher les résultats\n",
    "            print(f\"\\n     Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "            print(f\"     Meilleur f1 score pour {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "            # Sauvegarder le meilleur modèle si nécessaire\n",
    "            best_model = grid_search.best_estimator_\n",
    "            filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "            pickle.dump(best_model, open(filename, \"wb\"))\n",
    "    else:\n",
    "        for p in percents:\n",
    "            df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train_\"+p+\"_percent.csv\")\n",
    "            df_train = df_train.sort_values(by=\"Heure\")\n",
    "        \n",
    "            X_train = df_train.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "\n",
    "            X_train = standardize_dataframe(X_train) # on standardise les données\n",
    "            y_train = df_train[\"FlagImpaye\"]\n",
    "\n",
    "            # Boucle sur chaque modèle\n",
    "            for model_name, model in models.items():\n",
    "                print(f\"\\nTraining {model_name} for method {m} and {p} % of frauds\")\n",
    "\n",
    "                # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "                param_grid = param_grids[model_name]\n",
    "                print(param_grid)\n",
    "\n",
    "                f1_scorer = make_scorer(f1_score,greater_is_better=True)\n",
    "\n",
    "                # Utiliser TimeSeriesSplit pour la validation croisée\n",
    "                tscv = TimeSeriesSplit(n_splits=4)\n",
    "                \n",
    "                \n",
    "                # Créer la grille de recherche avec votre fonction personnalisée comme mesure d'évaluation\n",
    "                grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=f1_scorer, cv=tscv, n_jobs=-1)\n",
    "\n",
    "                # Effectuer la recherche de grille\n",
    "                grid_search.fit(X_train, y_train)\n",
    "\n",
    "                \n",
    "                # Afficher les résultats\n",
    "                print(f\"\\n     Best parameters for {model_name}: \", grid_search.best_params_)\n",
    "                print(f\"     Meilleur f1 score pour {model_name}: \", grid_search.best_score_)\n",
    "\n",
    "                # Sauvegarder le meilleur modèle si nécessaire\n",
    "                best_model = grid_search.best_estimator_\n",
    "                filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "                pickle.dump(best_model, open(filename, \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINNING : GRID SEARCH - MARGE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# from sklearn.metrics import f1_score \n",
    "# from sklearn.model_selection import TimeSeriesSplit\n",
    "# import pickle\n",
    "# from itertools import product\n",
    "# import numpy as np\n",
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# for m in methods:\n",
    "#     if m == \"simple\":\n",
    "#         df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train.csv\")\n",
    "#         for model_name, model in models.items():\n",
    "#                 print(f\"\\nTraining {model_name} for method {m}\")\n",
    "\n",
    "#                 # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "#                 param_grid = param_grids[model_name]\n",
    "\n",
    "#                 # Initialiser les variables pour stocker les meilleurs paramètres et le meilleur score\n",
    "#                 best_params = None\n",
    "#                 best_score = 0\n",
    "\n",
    "#                 # Effectuer une recherche par grille manuelle\n",
    "#                 for params in product(*param_grid.values()):\n",
    "#                     param_dict = dict(zip(param_grid.keys(), params))\n",
    "#                     print(param_dict)\n",
    "\n",
    "#                     # Initialiser le modèle avec les paramètres actuels\n",
    "#                     clf = model.set_params(**param_dict)\n",
    "\n",
    "#                     # Effectuer une validation croisée avec TimeSeriesSplit\n",
    "#                     tscv = TimeSeriesSplit(n_splits=4)\n",
    "#                     scores = []\n",
    "                    \n",
    "#                     df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "#                     X_train = df_train_sorted.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "#                     y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "                    \n",
    "#                     for train_index, val_index in tscv.split(X_train):\n",
    "#                         X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "#                         y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "#                         # train :\n",
    "#                         X_train_fold = standardize_dataframe(X_train_fold) # on standardise les données*\n",
    "                        \n",
    "#                         #test :\n",
    "#                         montants = X_val_fold[\"Montant\"]\n",
    "#                         montants = montants.values\n",
    "#                         X_val_fold = standardize_dataframe(X_val_fold) # on standardise les données*\n",
    "\n",
    "#                         # Entraîner le modèle\n",
    "#                         clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "#                         # Prédire sur l'ensemble de validation\n",
    "#                         y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "#                         y_val_fold = y_val_fold.values\n",
    "#                         y_val_pred = y_val_pred.tolist()     \n",
    "#                         y_val_fold = y_val_fold.tolist()\n",
    "#                         montants = montants.tolist()\n",
    "\n",
    "                        \n",
    "#                         score_marge = custom_score(y_val_fold, y_val_pred,montants)        \n",
    "#                         print(\"  marge : \",score_marge)\n",
    "#                         scores.append(score_marge)\n",
    "                    \n",
    "#                     # Calculer le score moyen sur les plis\n",
    "#                     avg_score = np.mean(scores)\n",
    "#                     print(\"   moyenne : \",avg_score)\n",
    "\n",
    "#                     if avg_score > best_score:\n",
    "#                         best_score = avg_score\n",
    "#                         best_params = param_dict\n",
    "#                         best_model = clf\n",
    "\n",
    "\n",
    "#                 # Afficher les résultats\n",
    "#                 print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "#                 #print(f\"     Best marge pour {model_name}: {best_score}\")\n",
    "\n",
    "#                 # Sauvegarder le meilleur modèle\n",
    "#                 filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "#                 pickle.dump(best_model, open(filename, \"wb\"))\n",
    "#     else:\n",
    "#         for p in percents:\n",
    "#             df_train = pd.read_csv(\"../data/\"+m+\"/dataframe_train_\"+p+\"_percent.csv\")\n",
    "\n",
    "#             # Boucle sur chaque modèle\n",
    "#             for model_name, model in models.items():\n",
    "#                 print(f\"\\nTraining {model_name} for method {m} and {p} % of frauds\")\n",
    "\n",
    "#                 # Définir les paramètres que vous souhaitez tester dans la recherche de grille\n",
    "#                 param_grid = param_grids[model_name]\n",
    "\n",
    "#                 # Initialiser les variables pour stocker les meilleurs paramètres et le meilleur score\n",
    "#                 best_params = None\n",
    "#                 best_score = 0\n",
    "\n",
    "#                 # Effectuer une recherche par grille manuelle\n",
    "#                 for params in product(*param_grid.values()):\n",
    "#                     param_dict = dict(zip(param_grid.keys(), params))\n",
    "#                     print(param_dict)\n",
    "\n",
    "#                     # Initialiser le modèle avec les paramètres actuels\n",
    "#                     clf = model.set_params(**param_dict)\n",
    "\n",
    "#                     # Effectuer une validation croisée avec TimeSeriesSplit\n",
    "#                     tscv = TimeSeriesSplit(n_splits=4)\n",
    "#                     scores = []\n",
    "                    \n",
    "#                     df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "#                     X_train = df_train_sorted.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "#                     y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "                    \n",
    "#                     for train_index, val_index in tscv.split(X_train):\n",
    "#                         X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "#                         y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "#                         # train :\n",
    "#                         X_train_fold = standardize_dataframe(X_train_fold) # on standardise les données*\n",
    "                        \n",
    "#                         #test :\n",
    "#                         montants = X_val_fold[\"Montant\"]\n",
    "#                         montants = montants.values\n",
    "#                         X_val_fold = standardize_dataframe(X_val_fold) # on standardise les données*\n",
    "\n",
    "#                         # Entraîner le modèle\n",
    "#                         clf.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "#                         # Prédire sur l'ensemble de validation\n",
    "#                         y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "#                         y_val_fold = y_val_fold.values\n",
    "#                         y_val_pred = y_val_pred.tolist()     \n",
    "#                         y_val_fold = y_val_fold.tolist()\n",
    "#                         montants = montants.tolist()\n",
    "\n",
    "                        \n",
    "#                         score_marge = custom_score(y_val_fold, y_val_pred,montants)        \n",
    "#                         print(\"  marge : \",score_marge)\n",
    "#                         scores.append(score_marge)\n",
    "                    \n",
    "#                     # Calculer le score moyen sur les plis\n",
    "#                     avg_score = np.mean(scores)\n",
    "#                     print(\"   moyenne : \",avg_score)\n",
    "\n",
    "#                     if avg_score > best_score:\n",
    "#                         best_score = avg_score\n",
    "#                         best_params = param_dict\n",
    "#                         best_model = clf\n",
    "\n",
    "\n",
    "#                 # Afficher les résultats\n",
    "#                 print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "#                 #print(f\"     Best marge pour {model_name}: {best_score}\")\n",
    "\n",
    "#                 # Sauvegarder le meilleur modèle\n",
    "#                 filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "#                 pickle.dump(best_model, open(filename, \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test parralele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Linear_Discriminant_Analysis': LinearDiscriminantAnalysis(), 'Logistic_Regression': LogisticRegression()}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import pickle\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "\n",
    "def train_model(method, model_name, models, param_grids, percents):\n",
    "    if method == \"simple\":\n",
    "        df_train = pd.read_csv(f\"../data/{method}/dataframe_train.csv\")\n",
    "        #df_train = df_train.head(20000)\n",
    "    else:\n",
    "        df_train = pd.read_csv(f\"../data/{method}/dataframe_train_{percents}_percent.csv\")\n",
    "        #df_train = df_train.head(20000)\n",
    "\n",
    "    print(f\"\\nTraining {model_name} for method {method}{' and ' + percents + '% of frauds' if method != 'simple' else ''}\")\n",
    "\n",
    "    param_grid = param_grids[model_name]\n",
    "    best_params = None\n",
    "    best_score = 0\n",
    "\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "\n",
    "        clf = models[model_name].set_params(**param_dict)\n",
    "        tscv = TimeSeriesSplit(n_splits=4)\n",
    "        scores = []\n",
    "\n",
    "        df_train_sorted = df_train.sort_values(by=\"Heure\")\n",
    "\n",
    "        X_train = df_train_sorted.drop(columns=[\"FlagImpaye\", \"CodeDecision\"])\n",
    "        y_train = df_train_sorted[\"FlagImpaye\"]\n",
    "\n",
    "        for train_index, val_index in tscv.split(X_train):\n",
    "            X_train_fold, X_val_fold = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "            y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "\n",
    "            X_train_fold = standardize_dataframe(X_train_fold)\n",
    "            montants = X_val_fold[\"Montant\"].values\n",
    "            X_val_fold = standardize_dataframe(X_val_fold)\n",
    "\n",
    "            clf.fit(X_train_fold, y_train_fold)\n",
    "            y_val_pred = clf.predict(X_val_fold)\n",
    "\n",
    "            y_val_fold, y_val_pred = y_val_fold.values.tolist(), y_val_pred.tolist()\n",
    "            montants = montants.tolist()\n",
    "\n",
    "            score_marge = custom_score(y_val_fold, y_val_pred, montants)\n",
    "            print(\"  marge : \", score_marge)\n",
    "            scores.append(score_marge)\n",
    "\n",
    "        avg_score = np.mean(scores)\n",
    "        print(\"   moyenne : \", avg_score)\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            best_params = param_dict\n",
    "            best_model = clf\n",
    "\n",
    "    print(f\"\\n     Best parameters pour {model_name}: {best_params}\")\n",
    "    filename = (\n",
    "        f\"../models/{method}/{percents}/{model_name}.pkl\"\n",
    "        if method != \"simple\"\n",
    "        else f\"../models/{method}/{model_name}.pkl\"\n",
    "    )\n",
    "    pickle.dump(best_model, open(filename, \"wb\"))\n",
    "\n",
    "\n",
    "print(models)\n",
    "Parallel(n_jobs=-1)(\n",
    "    delayed(train_model)(method, model_name, models, param_grids, p)\n",
    "    for method in methods\n",
    "    for model_name in models\n",
    "    for p in percents if method != \"simple\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "loaded_models = {method: {percent: None for percent in percents} for method in methods}\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        loaded_models[m] = {}\n",
    "        for model_name in models.keys():\n",
    "            filename = '../models/'+m+\"/\"+ model_name + '.pkl'\n",
    "            with open(filename, 'rb') as file:\n",
    "                loaded_models[m][model_name] = pickle.load(file)\n",
    "    else:\n",
    "        for p in percents:\n",
    "            loaded_models[m][p] = {}\n",
    "            for model_name in models.keys():\n",
    "                filename = '../models/'+m+\"/\"+p+\"/\"+ model_name + '.pkl'\n",
    "                with open(filename, 'rb') as file:\n",
    "                    loaded_models[m][p][model_name] = pickle.load(file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREDICTING WITH BEST MARGE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method simple ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.046928428251385455\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1975999.31\n",
      "     Montant gagné avec le modèle : 34147.62\n",
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method undersampling and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.017435845743298088\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1606248.53\n",
      "     Montant gagné avec le modèle : -335603.16\n",
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method undersampling and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.017435915120278103\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1606296.06\n",
      "     Montant gagné avec le modèle : -335555.63\n",
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method undersampling and 5 % of frauds ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.0174361463808656\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1606384.86\n",
      "     Montant gagné avec le modèle : -335466.83\n",
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method smote and 1 % of frauds ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.017436447028800775\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1606289.15\n",
      "     Montant gagné avec le modèle : -335562.54\n",
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method smote and 3 % of frauds ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.017440448946289627\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1606586.59\n",
      "     Montant gagné avec le modèle : -335265.1\n",
      "\n",
      " #### Testing Linear_Discriminant_Analysis for method smote and 5 % of frauds ####\n",
      "\n",
      "     F1 score de  Linear_Discriminant_Analysis  sur l'ensemble de test : 0.01744417492476155\n",
      "     Marge de  Linear_Discriminant_Analysis  sur l'ensemble de test : 1606807.49\n",
      "     Montant gagné avec le modèle : -335044.2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "index_modified = []\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        index_modified.append(m)\n",
    "    else:\n",
    "        for p in percents:\n",
    "            index_modified.append(m+\"_\"+p)\n",
    "  \n",
    "\n",
    "f1_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "marge_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "montant_gagne_df = pd.DataFrame(index=index_modified, columns=models.keys())\n",
    "\n",
    "df_test = pd.read_csv(\"../data/simple/dataframe_test.csv\")\n",
    "X_test = df_test.drop(columns=[\"FlagImpaye\",\"CodeDecision\"])\n",
    "montants = X_test[\"Montant\"]\n",
    "montants = montants.values\n",
    "montants = montants.tolist()\n",
    "\n",
    "X_test = standardize_dataframe(X_test) # on standardise les données test\n",
    "\n",
    "y_test = df_test[\"FlagImpaye\"]\n",
    "y_test = y_test.values\n",
    "y_test = y_test.tolist()\n",
    "\n",
    "for m in methods:\n",
    "    if m == \"simple\":\n",
    "        for model_name, model in loaded_models.get(m, {}).items():\n",
    "            print(f\"\\n #### Testing {model_name} for method {m} ####\")\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = y_pred.tolist()\n",
    "\n",
    "            nom_ligne = m\n",
    "\n",
    "            # f1 score\n",
    "            f1 = f1_score(y_test, y_pred)\n",
    "            f1_df.loc[nom_ligne, model_name] = f1\n",
    "\n",
    "            # marge\n",
    "            score_marge = custom_score(y_test, y_pred,montants)   \n",
    "            score_marge = round(score_marge,2)\n",
    "            marge_df.loc[nom_ligne, model_name] = score_marge\n",
    "\n",
    "         \n",
    "            marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "            montant_gagne = round(score_marge - marge_laisse_passer_tout_le_monde,2)\n",
    "            montant_gagne_df.at[nom_ligne ,model_name] = montant_gagne\n",
    "           \n",
    "\n",
    "            # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "            # disp = ConfusionMatrixDisplay(conf_matrix, display_labels=[False, True])\n",
    "            # disp.plot()\n",
    "            # plt.show()\n",
    "\n",
    "            print(\"\\n     F1 score de \",model_name,\" sur l'ensemble de test :\", f1)\n",
    "            print(\"     Marge de \",model_name,\" sur l'ensemble de test :\", score_marge)\n",
    "            print(\"     Montant gagné avec le modèle :\", montant_gagne)\n",
    "            \n",
    "    else:\n",
    "        for p, model_dic in loaded_models.get(m, {}).items():\n",
    "            for model_name, model in model_dic.items():\n",
    "                print(f\"\\n #### Testing {model_name} for method {m} and {p} % of frauds ####\")\n",
    "\n",
    "                y_pred = model.predict(X_test)\n",
    "                y_pred = y_pred.tolist()\n",
    "\n",
    "                nom_ligne = m+\"_\"+p\n",
    "\n",
    "                # f1 score\n",
    "                f1 = f1_score(y_test, y_pred)\n",
    "                f1_df.loc[nom_ligne, model_name] = f1\n",
    "\n",
    "                # marge\n",
    "                score_marge = custom_score(y_test, y_pred,montants)   \n",
    "                score_marge = round(score_marge,2)\n",
    "                marge_df.loc[nom_ligne, model_name] = score_marge\n",
    "\n",
    "                marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "                montant_gagne = round(score_marge - marge_laisse_passer_tout_le_monde,2)\n",
    "                montant_gagne_df.at[nom_ligne ,model_name] = montant_gagne\n",
    "        \n",
    "                # conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "                # disp = ConfusionMatrixDisplay(conf_matrix, display_labels=[False, True])\n",
    "                # disp.plot()\n",
    "                # plt.show()\n",
    "\n",
    "                print(\"\\n     F1 score de \",model_name,\" sur l'ensemble de test :\", f1)\n",
    "                print(\"     Marge de \",model_name,\" sur l'ensemble de test :\", score_marge)\n",
    "                print(\"     Montant gagné avec le modèle :\", montant_gagne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marge Parfaite :  2294459.36\n",
      "Marge si on laisse passer tout le monde :  1941851.69\n"
     ]
    }
   ],
   "source": [
    "marge_parfaite = round(custom_score(y_test, y_test,montants),2)\n",
    "marge_laisse_passer_tout_le_monde = round(custom_score(y_test,[0]*len(y_test),montants),2)\n",
    "\n",
    "print(\"Marge Parfaite : \",marge_parfaite)\n",
    "print(\"Marge si on laisse passer tout le monde : \",marge_laisse_passer_tout_le_monde)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Marge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear_Discriminant_Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>1975999.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>1606248.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>1606296.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_5</th>\n",
       "      <td>1606384.86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_1</th>\n",
       "      <td>1606289.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_3</th>\n",
       "      <td>1606586.59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_5</th>\n",
       "      <td>1606807.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Linear_Discriminant_Analysis\n",
       "simple                            1975999.31\n",
       "undersampling_1                   1606248.53\n",
       "undersampling_3                   1606296.06\n",
       "undersampling_5                   1606384.86\n",
       "smote_1                           1606289.15\n",
       "smote_3                           1606586.59\n",
       "smote_5                           1606807.49"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marge_df.to_csv('../data/marge.csv')\n",
    "marge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Montant gagné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear_Discriminant_Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>34147.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>-335603.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>-335555.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_5</th>\n",
       "      <td>-335466.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_1</th>\n",
       "      <td>-335562.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_3</th>\n",
       "      <td>-335265.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_5</th>\n",
       "      <td>-335044.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Linear_Discriminant_Analysis\n",
       "simple                              34147.62\n",
       "undersampling_1                   -335603.16\n",
       "undersampling_3                   -335555.63\n",
       "undersampling_5                   -335466.83\n",
       "smote_1                           -335562.54\n",
       "smote_3                            -335265.1\n",
       "smote_5                            -335044.2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "montant_gagne_df.to_csv('../data/montant_gagne.csv')\n",
    "montant_gagne_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Linear_Discriminant_Analysis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>simple</th>\n",
       "      <td>0.046928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_1</th>\n",
       "      <td>0.017436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_3</th>\n",
       "      <td>0.017436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undersampling_5</th>\n",
       "      <td>0.017436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_1</th>\n",
       "      <td>0.017436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_3</th>\n",
       "      <td>0.01744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smote_5</th>\n",
       "      <td>0.017444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Linear_Discriminant_Analysis\n",
       "simple                              0.046928\n",
       "undersampling_1                     0.017436\n",
       "undersampling_3                     0.017436\n",
       "undersampling_5                     0.017436\n",
       "smote_1                             0.017436\n",
       "smote_3                              0.01744\n",
       "smote_5                             0.017444"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_df.to_csv('../data/f1_score.csv')\n",
    "f1_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
